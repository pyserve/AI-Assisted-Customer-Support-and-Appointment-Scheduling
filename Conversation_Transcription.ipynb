{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH-gHA6r2EHR",
        "outputId": "35ffa625-c6d9-4fdb-d516-2d88ac65e09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "source": [
        "num_speakers = 2 #@param {type:\"integer\"}\n",
        "language = 'English' #@param ['any', 'English']\n",
        "model_size = 'large' #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
        "model_name = model_size\n",
        "if language == 'English' and model_size != 'large':\n",
        "  model_name += '.en'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "5HZPlMm7k9Pf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kniqsWvI9E3J"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/openai/whisper.git > /dev/null\n",
        "!pip install -q git+https://github.com/pyannote/pyannote-audio > /dev/null\n",
        "\n",
        "import whisper\n",
        "import datetime\n",
        "\n",
        "import subprocess\n",
        "\n",
        "import torch\n",
        "import pyannote.audio\n",
        "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding\n",
        "embedding_model = PretrainedSpeakerEmbedding(\n",
        "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    device=torch.device(\"cuda\"))\n",
        "\n",
        "from pyannote.audio import Audio\n",
        "from pyannote.core import Segment\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/call_logs/'"
      ],
      "metadata": {
        "id": "_1tSi5S7X9kV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import contextlib\n",
        "import wave\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "jLHzcCegYSW3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBe7sTmX9GWQ"
      },
      "outputs": [],
      "source": [
        "# model = whisper.load_model(model_size)\n",
        "\n",
        "def segment_embedding(segment, audio, path, duration):\n",
        "    start = segment[\"start\"]\n",
        "    end = min(duration, segment[\"end\"])\n",
        "    clip = Segment(start, end)\n",
        "    waveform, sample_rate = audio.crop(path, clip)\n",
        "    return embedding_model(waveform[None])\n",
        "\n",
        "def time(secs):\n",
        "    return str(datetime.timedelta(seconds=round(secs)))\n",
        "\n",
        "# Create the transcriptions folder if it doesn't exist\n",
        "if not os.path.exists('/content/drive/MyDrive/transcriptions'):\n",
        "    os.makedirs('/content/drive/MyDrive/transcriptions')\n",
        "\n",
        "# Process each call recording in the specified directory\n",
        "directory = 'path_to_call_recordings'  # Replace with the directory containing your call recordings\n",
        "directory = '/content/drive/MyDrive/call_logs/'\n",
        "for filename in os.listdir(directory)[3:]:\n",
        "    if filename.endswith(('.wav', '.mp3', '.mp4', '.m4a', '.flac')):\n",
        "        path = os.path.join(directory, filename)\n",
        "        if os.path.getsize(path) < 100 * 1024:\n",
        "            continue\n",
        "\n",
        "        # Convert to WAV if necessary\n",
        "        if not filename.endswith('.wav'):\n",
        "            subprocess.call(['ffmpeg', '-i', path, '/content/drive/MyDrive/call_logs/audio.wav', '-y'])\n",
        "            path = '/content/drive/MyDrive/call_logs/audio.wav'\n",
        "\n",
        "        # Transcribe and diarize the audio\n",
        "        result = model.transcribe(path)\n",
        "        segments = result[\"segments\"]\n",
        "        with contextlib.closing(wave.open(path,'r')) as f:\n",
        "            frames = f.getnframes()\n",
        "            rate = f.getframerate()\n",
        "            duration = frames / float(rate)\n",
        "\n",
        "        audio = Audio()\n",
        "        embeddings = np.zeros(shape=(len(segments), 192))\n",
        "        for i, segment in enumerate(segments):\n",
        "            embeddings[i] = segment_embedding(segment, audio, path, duration)\n",
        "\n",
        "        embeddings = np.nan_to_num(embeddings)\n",
        "        clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "        labels = clustering.labels_\n",
        "        for i in range(len(segments)):\n",
        "            segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "        # Write the transcription to a CSV file\n",
        "        transcript_data = []\n",
        "        for (i, segment) in enumerate(segments):\n",
        "            if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "                transcript_data.append([segment[\"speaker\"], time(segment[\"start\"]), segment[\"text\"]])\n",
        "            else:\n",
        "                transcript_data[-1][2] += ' ' + segment[\"text\"]\n",
        "\n",
        "        df = pd.DataFrame(transcript_data, columns=['Speaker', 'Time', 'Text'])\n",
        "        output_file = os.path.join('/content/drive/MyDrive/transcriptions', f'{os.path.splitext(filename)[0]}_transcription.csv')\n",
        "        df.to_csv(output_file, index=False)\n",
        "\n",
        "        # Clean up temporary files\n",
        "        if path == '/content/drive/MyDrive/call_logs/audio.wav':\n",
        "            os.remove('/content/drive/MyDrive/call_logs/audio.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dajCvnfP9OYW",
        "outputId": "8c5c971e-5e94-43b6-f0d5-1f47ea853888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "195\n",
            "Analysis complete and saved to: /content/drive/MyDrive/Call_Recordings_Analysis_9900.xlsx\n"
          ]
        }
      ],
      "source": [
        "# df = pd.DataFrame(data)\n",
        "# # Save the DataFrame to an Excel file\n",
        "# excel_file = '/content/drive/MyDrive/Call_Recordings_Analysis_9900.xlsx'\n",
        "# df.to_excel(excel_file, index=False)\n",
        "# print(len(df))\n",
        "# print(\"Analysis complete and saved to:\", excel_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if path[-3:] != 'wav':\n",
        "#   subprocess.call(['ffmpeg', '-i', path, 'audio.wav', '-y'])\n",
        "#   path = 'audio.wav'\n",
        "\n",
        "# model = whisper.load_model(model_size)\n",
        "# result = model.transcribe(path)\n",
        "# segments = result[\"segments\"]\n",
        "# with contextlib.closing(wave.open(path,'r')) as f:\n",
        "#   frames = f.getnframes()\n",
        "#   rate = f.getframerate()\n",
        "#   duration = frames / float(rate)\n",
        "# audio = Audio()\n",
        "\n",
        "# def segment_embedding(segment):\n",
        "#   start = segment[\"start\"]\n",
        "#   # Whisper overshoots the end timestamp in the last segment\n",
        "#   end = min(duration, segment[\"end\"])\n",
        "#   clip = Segment(start, end)\n",
        "#   waveform, sample_rate = audio.crop(path, clip)\n",
        "#   return embedding_model(waveform[None])\n",
        "\n",
        "# embeddings = np.zeros(shape=(len(segments), 192))\n",
        "# for i, segment in enumerate(segments):\n",
        "#   embeddings[i] = segment_embedding(segment)\n",
        "\n",
        "# embeddings = np.nan_to_num(embeddings)\n",
        "\n",
        "# clustering = AgglomerativeClustering(num_speakers).fit(embeddings)\n",
        "# labels = clustering.labels_\n",
        "# for i in range(len(segments)):\n",
        "#   segments[i][\"speaker\"] = 'SPEAKER ' + str(labels[i] + 1)\n",
        "\n",
        "# def time(secs):\n",
        "#   return datetime.timedelta(seconds=round(secs))\n",
        "\n",
        "# f = open(\"transcript.txt\", \"w\")\n",
        "\n",
        "# for (i, segment) in enumerate(segments):\n",
        "#   if i == 0 or segments[i - 1][\"speaker\"] != segment[\"speaker\"]:\n",
        "#     f.write(\"\\n\" + segment[\"speaker\"] + ' ' + str(time(segment[\"start\"])) + '\\n')\n",
        "#   f.write(segment[\"text\"][1:] + ' ')\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "HO7NHMz4SjN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5TleZZg7ti1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
