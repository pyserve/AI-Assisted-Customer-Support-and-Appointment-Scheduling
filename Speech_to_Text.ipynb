{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH-gHA6r2EHR",
        "outputId": "f5b23bc6-7661-4e06-f411-ae9900bbd22c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kniqsWvI9E3J",
        "outputId": "8cd5b567-f354-4ffa-ad74-a3871f65ebc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import whisper\n",
        "import librosa\n",
        "import numpy as np\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBe7sTmX9GWQ"
      },
      "outputs": [],
      "source": [
        "# Define the path to your folder in Google Drive\n",
        "folder_path = '/content/drive/MyDrive/Call Recordings'  # Update this path accordingly\n",
        "\n",
        "# Get the list of audio files in the folder\n",
        "audio_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "# Load the model for transcription\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe the audio file\n",
        "def transcribe_audio(audio_file_path):\n",
        "    result = model.transcribe(audio_file_path)\n",
        "    return result[\"text\"]\n",
        "\n",
        "# Extract audio features\n",
        "def extract_audio_features(audio_file_path):\n",
        "    y, sr = librosa.load(audio_file_path)\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "    amplitude_mean = np.mean(y)\n",
        "    amplitude_max = np.max(y)\n",
        "    amplitude_min = np.min(y)\n",
        "    amplitude_std = np.std(y)\n",
        "    non_silent_intervals = librosa.effects.split(y, top_db=20)\n",
        "    total_silence_duration = duration - sum((e - s) for s, e in non_silent_intervals) / sr\n",
        "\n",
        "    return {\n",
        "        \"duration\": duration,\n",
        "        \"amplitude_mean\": amplitude_mean,\n",
        "        \"amplitude_max\": amplitude_max,\n",
        "        \"amplitude_min\": amplitude_min,\n",
        "        \"amplitude_std\": amplitude_std,\n",
        "        \"total_silence_duration\": total_silence_duration\n",
        "    }\n",
        "\n",
        "# Analyze the transcription\n",
        "def analyze_transcription(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment\n",
        "    return {\n",
        "        \"sentiment_polarity\": sentiment.polarity,\n",
        "        \"sentiment_subjectivity\": sentiment.subjectivity\n",
        "    }\n",
        "\n",
        "# Compile features into a DataFrame\n",
        "def compile_features(audio_files):\n",
        "    data = []\n",
        "    for audio_file in audio_files:\n",
        "        features = {}\n",
        "        features['file_name'] = os.path.basename(audio_file)\n",
        "\n",
        "        # Transcribe the audio\n",
        "        transcription = transcribe_audio(audio_file)\n",
        "        features['transcription'] = transcription\n",
        "\n",
        "        # Extract audio features\n",
        "        audio_features = extract_audio_features(audio_file)\n",
        "        features.update(audio_features)\n",
        "\n",
        "        # Analyze the transcription\n",
        "        transcription_analysis = analyze_transcription(transcription)\n",
        "        features.update(transcription_analysis)\n",
        "\n",
        "        data.append(features)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "# Process the audio files and compile the DataFrame\n",
        "df = compile_features(audio_files[:500])\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "excel_file = '/content/drive/MyDrive/Call_Recordings_Analysis_500.xlsx'\n",
        "df.to_excel(excel_file, index=False)\n",
        "\n",
        "print(\"Analysis complete and saved to:\", excel_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibkl_SJXLuMM"
      },
      "outputs": [],
      "source": [
        "topic modelling with NE\n",
        "time span of data \n",
        "subsample data to each week less data but larger timespan\n",
        "preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seasonality patterns \n",
        "categorization of conversations \n",
        "subsample based on topic \n",
        "EDA based on categorization \n",
        "conversation label "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To do Tomorrow\n",
        "- Speech to Text (accurately)\n",
        "- Text to Speech (accurately)\n",
        "- Subsample data from various time \n",
        "- Topic Modelling \n",
        "- Book appointment \n",
        "- LLM model \n",
        "- Response vs Input \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Pitch of the speaker (text to speech limitations)\n",
        "# Overlapping voices (Parsing Data)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
